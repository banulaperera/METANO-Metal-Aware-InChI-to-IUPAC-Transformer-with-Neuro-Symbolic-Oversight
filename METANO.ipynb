{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banulaperera/METANO-Metal-Aware-InChI-to-IUPAC-Transformer-with-Neuro-Symbolic-Oversight/blob/model_dev/METANO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_overview"
      },
      "source": [
        "# METANO: Metal Aware InChI to IUPAC Transformer with Neuro Symbolic Oversight\n",
        "\n",
        "## Project Overview\n",
        "METANO is an AI-driven software platform designed to automatically translate International Chemical Identifier (InChI) strings, including challenging Reconnect InChI variants, into systematic IUPAC names. This implementation leverages:\n",
        "\n",
        "- **Pre-trained T5 Transformer Model** for sequence-to-sequence translation\n",
        "- **Neurosymbolic Validation** combining neural predictions with chemical rules\n",
        "- **Comprehensive Accuracy Evaluation** with multiple metrics\n",
        "\n",
        "### Key Features\n",
        "- Handles both Standard InChI and Reconnected InChI formats  \n",
        "- Optimized for organometallic and coordination compounds  \n",
        "- Real-time chemical validation and error correction  \n",
        "- Production-ready accuracy metrics and evaluation  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "##  Setup and Installation\n",
        "Install required dependencies and configure the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l1PIm2qKpyh"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install RDKit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Transformers and ML libraries\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Chemical libraries\n",
        "try:\n",
        "    from rdkit import Chem\n",
        "    from rdkit.Chem import Descriptors\n",
        "    print(\"‚úÖ RDKit loaded successfully\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è RDKit not available - chemical validation will be limited\")\n",
        "    Chem = None\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_section"
      },
      "source": [
        "##  Configuration and Hyperparameters\n",
        "Optimized configuration for production-level performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "configuration"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class InChI2NameConfig:\n",
        "    \"\"\"Optimized configuration for InChI2Name model\"\"\"\n",
        "\n",
        "    # Model parameters\n",
        "    model_name: str = \"t5-small\"  # Pre-trained T5 model\n",
        "    max_input_length: int = 512\n",
        "    max_output_length: int = 256\n",
        "\n",
        "    # Training hyperparameters (optimized)\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 0.01\n",
        "    warmup_ratio: float = 0.1\n",
        "    num_epochs: int = 10\n",
        "    batch_size: int = 4  # Reduced batch size to save memory\n",
        "    gradient_accumulation_steps: int = 8 # Increased accumulation steps to compensate\n",
        "\n",
        "    # Validation parameters\n",
        "    eval_steps: int = 500\n",
        "    save_steps: int = 1000\n",
        "    early_stopping_patience: int = 3\n",
        "\n",
        "    # Generation parameters\n",
        "    num_beams: int = 4\n",
        "    do_sample: bool = False\n",
        "    temperature: float = 1.0\n",
        "    top_k: int = 50\n",
        "\n",
        "    # Neurosymbolic parameters\n",
        "    enable_chemical_validation: bool = True\n",
        "    enable_grammar_checking: bool = True\n",
        "    confidence_threshold: float = 0.8\n",
        "\n",
        "    # Data parameters\n",
        "    train_split: float = 0.8\n",
        "    val_split: float = 0.1\n",
        "    test_split: float = 0.1\n",
        "\n",
        "    # Output directory\n",
        "    output_dir: str = \"./inchi2name_model\"\n",
        "\n",
        "# Initialize configuration\n",
        "config = InChI2NameConfig()\n",
        "print(\"üìã Configuration loaded:\")\n",
        "print(f\"   Model: {config.model_name}\")\n",
        "print(f\"   Learning rate: {config.learning_rate}\")\n",
        "print(f\"   Batch size: {config.batch_size}\")\n",
        "print(f\"   Max epochs: {config.num_epochs}\")\n",
        "print(f\"   Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_section"
      },
      "source": [
        "##  Data Loading and Preprocessing\n",
        "Load and preprocess the InChI-IUPAC dataset with robust validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loader"
      },
      "outputs": [],
      "source": [
        "class InChIDatasetLoader:\n",
        "    \"\"\"Enhanced dataset loader with chemical validation\"\"\"\n",
        "\n",
        "    def __init__(self, config: InChI2NameConfig):\n",
        "        self.config = config\n",
        "        self.metal_elements = [\n",
        "            'Li', 'Na', 'K', 'Rb', 'Cs', 'Be', 'Mg', 'Ca', 'Sr', 'Ba', 'Ra',\n",
        "            'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',\n",
        "            'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd',\n",
        "            'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg',\n",
        "            'Al', 'Ga', 'In', 'Tl', 'Sn', 'Pb', 'Bi'\n",
        "        ]\n",
        "\n",
        "    def create_sample_dataset(self) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Create a sample dataset for demonstration\"\"\"\n",
        "        sample_data = [\n",
        "            # Organic compounds\n",
        "            (\"InChI=1S/CH4/h1H4\", \"methane\"),\n",
        "            (\"InChI=1S/C2H6/c1-2/h1-2H3\", \"ethane\"),\n",
        "            (\"InChI=1S/C2H6O/c1-2-3/h3H,2H2,1H3\", \"ethanol\"),\n",
        "            (\"InChI=1S/C6H6/c1-2-4-6-5-3-1/h1-6H\", \"benzene\"),\n",
        "            (\"InChI=1S/C7H8/c1-7-5-3-2-4-6-7/h2-6H,1H3\", \"toluene\"),\n",
        "            (\"InChI=1S/C8H10/c1-3-7-5-4-6-8(2)9-7/h4-6H,3H2,1-2H3\", \"ethylbenzene\"),\n",
        "\n",
        "            # Organometallic compounds (reconnected InChI)\n",
        "            (\"InChI=1/2C5H5.Fe/c2*1-2-4-5-3-1;/h2*1-5H;/rC10H10Fe/c1-2-4-8-6(1)11-7-3-1-5-9(7)10(11)12-8/h1-10H\",\n",
        "             \"bis(Œ∑‚Åµ-cyclopentadienyl)iron\"),\n",
        "            (\"InChI=1/4CO.Co/c4*1-2;/rC4CoO4/c6-1-5(2-7,3-8)4-9/o1-2,2-3,3-4,4-1\",\n",
        "             \"tetracarbonylcobalt(0)\"),\n",
        "            (\"InChI=1/6CO.Cr/c6*1-2;/rC6CrO6/c7-1-8(2-9,3-10,4-11,5-12)6-13\",\n",
        "             \"hexacarbonylchromium(0)\"),\n",
        "            (\"InChI=1/4C18H15P.Ni/c4*1-4-10-16(11-5-1)19(17-12-6-2-7-13-17)18-14-8-3-9-15-18;/h4*1-15H;\",\n",
        "             \"tetrakis(triphenylphosphine)nickel(0)\"),\n",
        "\n",
        "            # Complex organics\n",
        "            (\"InChI=1S/C9H8O/c10-8-7-9-5-3-1-2-4-6-9/h1-8H\", \"benzaldehyde\"),\n",
        "            (\"InChI=1S/C7H6O2/c8-6-7-4-2-1-3-5-7-9/h1-5H,(H,8,9)\", \"benzoic acid\"),\n",
        "            (\"InChI=1S/C6H12O6/c7-1-2-3(8)4(9)5(10)6(11)12-2/h2-11H,1H2/t2-,3-,4+,5-,6+/m1/s1\",\n",
        "             \"Œ±-D-glucopyranose\"),\n",
        "\n",
        "            # Coordination compounds\n",
        "            (\"InChI=1/6H3N.Co.3ClH/h6*1H3;;3*1H/q;;3*-1;+3\", \"hexaamminecobalt(III) chloride\"),\n",
        "            (\"InChI=1/2C10H8N2.Fe/c2*1-3-9-5-6-10-4-2-8-12-11-9;/h2*1-8H;\",\n",
        "             \"bis(2,2'-bipyridine)iron(II)\"),\n",
        "\n",
        "            # More organometallics\n",
        "            (\"InChI=1/C8H8.Fe/c1-2-4-6-8-7-5-3-1;/h1-8H;\", \"(Œ∑‚Å∏-cyclooctatetraene)iron\"),\n",
        "        ]\n",
        "\n",
        "        # Expand dataset with variations\n",
        "        expanded_data = sample_data * 50  # Duplicate for training\n",
        "        random.shuffle(expanded_data)\n",
        "\n",
        "        return expanded_data\n",
        "\n",
        "    def load_from_file(self, file_path: str) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Load dataset from tab-separated file\"\"\"\n",
        "        data = []\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"üìÅ File not found: {file_path}\")\n",
        "            print(\"üìù Creating sample dataset for demonstration...\")\n",
        "            return self.create_sample_dataset()\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            print(f\"üìÇ Loading dataset from {file_path}\")\n",
        "            valid_count = 0\n",
        "            invalid_count = 0\n",
        "\n",
        "            for line_num, line in enumerate(tqdm(lines, desc=\"Processing lines\"), 1):\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith('#'):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    parts = line.split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        inchi = parts[0].strip()\n",
        "                        iupac = parts[1].strip()\n",
        "\n",
        "                        if self._validate_pair(inchi, iupac):\n",
        "                            data.append((inchi, iupac))\n",
        "                            valid_count += 1\n",
        "                        else:\n",
        "                            invalid_count += 1\n",
        "                except Exception as e:\n",
        "                    invalid_count += 1\n",
        "                    continue\n",
        "\n",
        "            print(f\"‚úÖ Loaded {valid_count} valid pairs\")\n",
        "            print(f\"‚ùå Filtered {invalid_count} invalid pairs\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading file: {e}\")\n",
        "            print(\"üìù Using sample dataset instead...\")\n",
        "            return self.create_sample_dataset()\n",
        "\n",
        "        return data if data else self.create_sample_dataset()\n",
        "\n",
        "    def _validate_pair(self, inchi: str, iupac: str) -> bool:\n",
        "        \"\"\"Validate InChI-IUPAC pair\"\"\"\n",
        "        # Basic format checks\n",
        "        if not inchi or not iupac:\n",
        "            return False\n",
        "\n",
        "        if not inchi.startswith('InChI='):\n",
        "            return False\n",
        "\n",
        "        if len(inchi) < 10 or len(iupac) < 2:\n",
        "            return False\n",
        "\n",
        "        # Check for invalid IUPAC names\n",
        "        invalid_names = ['unknown', 'error', 'invalid', 'n/a', 'none']\n",
        "        if iupac.lower().strip() in invalid_names:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def analyze_dataset(self, data: List[Tuple[str, str]]) -> Dict:\n",
        "        \"\"\"Analyze dataset composition\"\"\"\n",
        "        analysis = {\n",
        "            'total_pairs': len(data),\n",
        "            'inchi_formats': {'standard': 0, 'reconnected': 0, 'unknown': 0},\n",
        "            'has_metals': 0,\n",
        "            'avg_inchi_length': 0,\n",
        "            'avg_iupac_length': 0,\n",
        "            'metal_distribution': {},\n",
        "            'sample_pairs': []\n",
        "        }\n",
        "\n",
        "        inchi_lengths = []\n",
        "        iupac_lengths = []\n",
        "\n",
        "        for i, (inchi, iupac) in enumerate(data):\n",
        "            inchi_lengths.append(len(inchi))\n",
        "            iupac_lengths.append(len(iupac))\n",
        "\n",
        "            # Format analysis\n",
        "            if 'InChI=1S/' in inchi:\n",
        "                analysis['inchi_formats']['standard'] += 1\n",
        "            elif '/r' in inchi:\n",
        "                analysis['inchi_formats']['reconnected'] += 1\n",
        "            else:\n",
        "                analysis['inchi_formats']['unknown'] += 1\n",
        "\n",
        "            # Metal analysis\n",
        "            metals_found = [m for m in self.metal_elements if m in inchi]\n",
        "            if metals_found:\n",
        "                analysis['has_metals'] += 1\n",
        "                for metal in metals_found:\n",
        "                    analysis['metal_distribution'][metal] = analysis['metal_distribution'].get(metal, 0) + 1\n",
        "\n",
        "            # Sample pairs\n",
        "            if i < 3:\n",
        "                analysis['sample_pairs'].append({\n",
        "                    'inchi': inchi[:60] + '...' if len(inchi) > 60 else inchi,\n",
        "                    'iupac': iupac\n",
        "                })\n",
        "\n",
        "        analysis['avg_inchi_length'] = np.mean(inchi_lengths) if inchi_lengths else 0\n",
        "        analysis['avg_iupac_length'] = np.mean(iupac_lengths) if iupac_lengths else 0\n",
        "\n",
        "        return analysis\n",
        "\n",
        "# Load and analyze dataset\n",
        "print(\"üìä Loading InChI-IUPAC dataset...\")\n",
        "loader = InChIDatasetLoader(config)\n",
        "\n",
        "# Try to load from file or create sample data\n",
        "dataset_path = \"/content/InorganicReconnectedNames.txt\"  # Replace with your actual dataset path\n",
        "data = loader.load_from_file(dataset_path)\n",
        "\n",
        "# Analyze dataset\n",
        "analysis = loader.analyze_dataset(data)\n",
        "print(f\"\\nüìà Dataset Analysis:\")\n",
        "print(f\"   Total pairs: {analysis['total_pairs']}\")\n",
        "print(f\"   InChI formats: {analysis['inchi_formats']}\")\n",
        "print(f\"   Contains metals: {analysis['has_metals']}\")\n",
        "print(f\"   Avg InChI length: {analysis['avg_inchi_length']:.1f}\")\n",
        "print(f\"   Avg IUPAC length: {analysis['avg_iupac_length']:.1f}\")\n",
        "\n",
        "if analysis['metal_distribution']:\n",
        "    top_metals = dict(sorted(analysis['metal_distribution'].items(), key=lambda x: x[1], reverse=True)[:5])\n",
        "    print(f\"   Top metals: {top_metals}\")\n",
        "\n",
        "print(f\"\\nüìù Sample pairs:\")\n",
        "for i, sample in enumerate(analysis['sample_pairs']):\n",
        "    print(f\"   {i+1}. InChI: {sample['inchi']}\")\n",
        "    print(f\"      IUPAC: {sample['iupac']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neurosymbolic_section"
      },
      "source": [
        "##  Neurosymbolic Validation System\n",
        "Implement chemical rule-based validation and correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neurosymbolic_validator"
      },
      "outputs": [],
      "source": [
        "class ChemicalValidator:\n",
        "    \"\"\"Neurosymbolic chemical validation system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "      # Metal coordination geometry rules\n",
        "      self.coordination_geometries = {\n",
        "            2: {'linear': [180], 'bent': [104, 120]},\n",
        "            3: {'trigonal_planar': [120], 'trigonal_pyramidal': [107]},\n",
        "            4: {'tetrahedral': [109.5], 'square_planar': [90]},\n",
        "            5: {'trigonal_bipyramidal': [90, 120], 'square_pyramidal': [90, 180]},\n",
        "            6: {'octahedral': [90, 180]},\n",
        "            8: {'cubic': [90], 'square_antiprismatic': [90, 180]}\n",
        "        }\n",
        "\n",
        "      # Metal oxidation states\n",
        "      self.common_oxidation_states = {\n",
        "          'Fe': [0, 2, 3, 6], 'Co': [0, 2, 3], 'Ni': [0, 2, 4],\n",
        "          'Cu': [0, 1, 2], 'Zn': [0, 2], 'Mn': [0, 2, 3, 4, 6, 7],\n",
        "          'Cr': [0, 2, 3, 6], 'Ti': [0, 2, 3, 4], 'V': [0, 2, 3, 4, 5],\n",
        "          'Pt': [0, 2, 4], 'Pd': [0, 2, 4], 'Au': [0, 1, 3],\n",
        "          'Ag': [0, 1], 'Ru': [0, 2, 3, 4, 6, 8], 'Rh': [0, 1, 3],\n",
        "          'Os': [0, 2, 3, 4, 6, 8], 'Ir': [0, 1, 3, 4, 6]\n",
        "      }\n",
        "      # Chemical grammar rules\n",
        "      self.grammar_rules = {\n",
        "          'required_patterns': [\n",
        "              r'^[a-zA-Z]',  # Must start with letter\n",
        "              r'[a-zA-Z0-9]$'  # Must end with alphanumeric\n",
        "          ],\n",
        "          'forbidden_patterns': [\n",
        "              r'--',  # Double dashes\n",
        "              r',,',  # Double commas\n",
        "              r'\\(\\)',  # Empty parentheses\n",
        "              r'\\[\\]',  # Empty brackets\n",
        "              r'  ',  # Double spaces\n",
        "          ],\n",
        "          'valid_chars': set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
        "                            '()[]{},-+‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ‚Å∫‚Åª‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚ÇâŒ∑Œ∫ŒºŒªŒîŒõ ')\n",
        "      }\n",
        "\n",
        "      # Stereochemistry validation\n",
        "      self.stereo_patterns = {\n",
        "          'chirality': [r'\\([RS]\\)', r'\\(R\\)', r'\\(S\\)'],\n",
        "          'geometry': [r'\\([EZ]\\)', r'\\(E\\)', r'\\(Z\\)'],\n",
        "          'coordination': [r'fac-', r'mer-', r'cis-', r'trans-'],\n",
        "          'haptic': [r'Œ∑[1-8]', r'Œ∑‚Åµ', r'Œ∑‚Å∂']\n",
        "      }\n",
        "\n",
        "      print(\" Enhanced Chemical Validator initialized\")\n",
        "      print(f\"   Coordination geometries: {len(self.coordination_geometries)}\")\n",
        "      print(f\"   Metal oxidation states: {len(self.common_oxidation_states)}\")\n",
        "\n",
        "      # self.metal_elements = [\n",
        "      #     'Li', 'Na', 'K', 'Rb', 'Cs', 'Be', 'Mg', 'Ca', 'Sr', 'Ba', 'Ra',\n",
        "      #     'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',\n",
        "      #     'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd',\n",
        "      #     'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg',\n",
        "      #     'Al', 'Ga', 'In', 'Tl', 'Sn', 'Pb', 'Bi'\n",
        "      # ]\n",
        "\n",
        "      # self.organometallic_terms = [\n",
        "      #     'cyclopentadienyl', 'carbonyl', 'bis', 'tris', 'tetrakis', 'pentakis',\n",
        "      #     'aqua', 'ammine', 'chloro', 'bromo', 'fluoro', 'iodo', 'cyano',\n",
        "      #     'Œ∑', 'Œ∫', 'Œº', 'fac', 'mer', 'cis', 'trans'\n",
        "      # ]\n",
        "\n",
        "    def validate_prediction(self, inchi: str, predicted_iupac: str) -> Dict:\n",
        "        \"\"\"Comprehensive validation of predicted IUPAC name\"\"\"\n",
        "        validation_result = {\n",
        "            'is_valid': True,\n",
        "            'confidence': 1.0,\n",
        "            'errors': [],\n",
        "            'warnings': [],\n",
        "            'scores': {},\n",
        "            'detailed_analysis': {}\n",
        "        }\n",
        "        # validation_result = {\n",
        "        #     'is_valid': True,\n",
        "        #     'confidence': 1.0,\n",
        "        #     'errors': [],\n",
        "        #     'warnings': [],\n",
        "        #     'chemical_consistency': 0.0,\n",
        "        #     'grammar_score': 0.0,\n",
        "        #     'metal_consistency': 0.0\n",
        "        # }\n",
        "\n",
        "        # Grammar validation\n",
        "        grammar_score = self._validate_grammar(predicted_iupac)\n",
        "        validation_result['grammar_score'] = grammar_score\n",
        "\n",
        "        if grammar_score < 0.8:\n",
        "            validation_result['errors'].append(\"Poor IUPAC grammar\")\n",
        "            validation_result['is_valid'] = False\n",
        "\n",
        "        # Metal consistency validation\n",
        "        metal_consistency = self._validate_metal_consistency(inchi, predicted_iupac)\n",
        "        validation_result['metal_consistency'] = metal_consistency\n",
        "\n",
        "        if metal_consistency < 0.7:\n",
        "            validation_result['warnings'].append(\"Metal consistency issues\")\n",
        "\n",
        "        # Chemical structure validation (if RDKit available)\n",
        "        if Chem:\n",
        "            chemical_consistency = self._validate_chemical_structure(inchi)\n",
        "            validation_result['chemical_consistency'] = chemical_consistency\n",
        "\n",
        "            if chemical_consistency < 0.5:\n",
        "                validation_result['errors'].append(\"Invalid chemical structure\")\n",
        "                validation_result['is_valid'] = False\n",
        "\n",
        "        # Calculate overall confidence\n",
        "        validation_result['confidence'] = (\n",
        "            grammar_score * 0.4 +\n",
        "            metal_consistency * 0.3 +\n",
        "            validation_result['chemical_consistency'] * 0.3\n",
        "        )\n",
        "\n",
        "        return validation_result\n",
        "\n",
        "    def _validate_grammar(self, iupac: str) -> float:\n",
        "        \"\"\"Validate IUPAC grammar rules\"\"\"\n",
        "        score = 1.0\n",
        "\n",
        "        # Character validation\n",
        "        invalid_chars = set(iupac) - self.grammar_rules['forbidden_patterns']\n",
        "        if invalid_chars:\n",
        "          score *= 0.3\n",
        "\n",
        "        # Length validation\n",
        "        if len(iupac.strip()) < 2:\n",
        "          score *= 0.1\n",
        "        # If the name is very long it can be malformed\n",
        "        elif len(iupac.strip()) > 200:\n",
        "          score *= 0.8\n",
        "\n",
        "        # Parenthese/bracket balance\n",
        "        if not self._check_bracket_balance(iupac):\n",
        "          score *= 0.4\n",
        "\n",
        "          return max(0.0, min(1.0, score))\n",
        "\n",
        "        # Required patterns\n",
        "        # score = 1.0\n",
        "\n",
        "        # # Basic character validation\n",
        "        # valid_chars = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
        "        # valid_chars.update('()[]{},-+‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ‚Å∫‚Åª‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚ÇâŒ∑Œ∫ŒºŒªŒîŒõ ')\n",
        "\n",
        "        # if not set(iupac).issubset(valid_chars):\n",
        "        #     score *= 0.5\n",
        "\n",
        "        # # Check for invalid patterns\n",
        "        # invalid_patterns = ['--', ',,', '  ', '()', '[]', '{}']\n",
        "        # for pattern in invalid_patterns:\n",
        "        #     if pattern in iupac:\n",
        "        #         score *= 0.8\n",
        "\n",
        "        # # Length validation\n",
        "        # if len(iupac.strip()) < 2:\n",
        "        #     score *= 0.3\n",
        "\n",
        "        # return score\n",
        "    def _check_bracket_balance(self, text: str) -> bool:\n",
        "        \"\"\"Check if parentheses and brackets are balanced \"\"\"\n",
        "        stack = []\n",
        "        pairs = {'(':')', '[':']', '{':'}'}\n",
        "\n",
        "        for char in text:\n",
        "            if char in pairs:\n",
        "                stack.append(char)\n",
        "            elif char in pairs.values():\n",
        "                if not stack or pairs[stack.pop()] != char:\n",
        "                    return False\n",
        "\n",
        "        return len(stack) == 0\n",
        "\n",
        "    def _validate_metal_consistency(self, inchi: str, iupac: str) -> float:\n",
        "        \"\"\"Validate metal consistency between InChI and IUPAC\"\"\"\n",
        "        inchi_metals = [m for m in self.metal_elements if m in inchi]\n",
        "\n",
        "        # Check for metal names in IUPAC\n",
        "        iupac_metals = []\n",
        "        iupac_lower = iupac.lower()\n",
        "\n",
        "        # Direct metal symbols\n",
        "        for metal in self.metal_elements:\n",
        "            if metal.lower() in iupac_lower:\n",
        "                iupac_metals.append(metal)\n",
        "\n",
        "        # Metal name variations\n",
        "        metal_names = {\n",
        "            'iron': 'Fe', 'ferr': 'Fe', 'cobalt': 'Co', 'nickel': 'Ni',\n",
        "            'copper': 'Cu', 'cupro': 'Cu', 'zinc': 'Zn', 'manganese': 'Mn',\n",
        "            'chromium': 'Cr', 'platinum': 'Pt', 'palladium': 'Pd'\n",
        "        }\n",
        "\n",
        "        for name, symbol in metal_names.items():\n",
        "            if name in iupac_lower:\n",
        "                iupac_metals.append(symbol)\n",
        "\n",
        "        # Calculate consistency\n",
        "        if not inchi_metals and not iupac_metals:\n",
        "            return 1.0  # No metals in either - consistent\n",
        "\n",
        "        if not inchi_metals or not iupac_metals:\n",
        "            return 0.3  # Mismatch\n",
        "\n",
        "        # Calculate overlap\n",
        "        common_metals = set(inchi_metals) & set(iupac_metals)\n",
        "        total_metals = set(inchi_metals) | set(iupac_metals)\n",
        "\n",
        "        return len(common_metals) / len(total_metals) if total_metals else 0.0\n",
        "\n",
        "    def _validate_chemical_structure(self, inchi: str) -> float:\n",
        "        \"\"\"Validate chemical structure using RDKit\"\"\"\n",
        "        if not Chem:\n",
        "            return 0.8  # Assume reasonable if RDKit not available\n",
        "\n",
        "        try:\n",
        "            mol = Chem.MolFromInchi(inchi)\n",
        "            if mol is None:\n",
        "                return 0.0\n",
        "\n",
        "            # Basic structure validation\n",
        "            num_atoms = mol.GetNumAtoms()\n",
        "            num_bonds = mol.GetNumBonds()\n",
        "\n",
        "            if num_atoms == 0:\n",
        "                return 0.0\n",
        "\n",
        "            # Calculate simple complexity score\n",
        "            complexity = min(1.0, (num_atoms + num_bonds) / 100.0)\n",
        "            return 0.7 + 0.3 * complexity\n",
        "\n",
        "        except Exception:\n",
        "            return 0.3\n",
        "\n",
        "# Initialize validator\n",
        "validator = ChemicalValidator()\n",
        "print(\"üß† Neurosymbolic validator initialized\")\n",
        "\n",
        "# Test validation\n",
        "test_inchi = \"InChI=1S/C6H6/c1-2-4-6-5-3-1/h1-6H\"\n",
        "test_iupac = \"benzene\"\n",
        "test_result = validator.validate_prediction(test_inchi, test_iupac)\n",
        "print(f\"\\nüß™ Test validation:\")\n",
        "print(f\"   Valid: {test_result['is_valid']}\")\n",
        "print(f\"   Confidence: {test_result['confidence']:.3f}\")\n",
        "print(f\"   Grammar score: {test_result['grammar_score']:.3f}\")\n",
        "print(f\"   Metal consistency: {test_result['metal_consistency']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_section"
      },
      "source": [
        "##  T5-Based InChI2Name Model\n",
        "Pre-trained T5 model with chemical-aware tokenization and neurosymbolic integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_definition",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class ChemicalT5Tokenizer:\n",
        "    \"\"\"Enhanced T5 tokenizer for chemical data\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # Add chemical special tokens\n",
        "        special_tokens = [\n",
        "            '<METAL>', '<STANDARD>', '<RECONNECTED>', '<ORGANIC>',\n",
        "            '<COORDINATION>', '<STEREO>', '<HAPTIC>'\n",
        "        ]\n",
        "\n",
        "        self.tokenizer.add_special_tokens({\n",
        "            'additional_special_tokens': special_tokens\n",
        "        })\n",
        "\n",
        "        print(f\"üî§ Tokenizer initialized with {len(self.tokenizer)} tokens\")\n",
        "\n",
        "    def preprocess_inchi(self, inchi: str) -> str:\n",
        "        \"\"\"Preprocess InChI with chemical markers\"\"\"\n",
        "        processed = inchi\n",
        "\n",
        "        # Add format markers\n",
        "        if 'InChI=1S/' in inchi:\n",
        "            processed = '<STANDARD> ' + processed\n",
        "        elif '/r' in inchi:\n",
        "            processed = '<RECONNECTED> ' + processed\n",
        "        else:\n",
        "            processed = '<ORGANIC> ' + processed\n",
        "\n",
        "        # Mark metals\n",
        "        metals = ['Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Mn', 'Cr', 'Pt', 'Pd']\n",
        "        for metal in metals:\n",
        "            if metal in processed:\n",
        "                processed = processed.replace(metal, f'<METAL> {metal}', 1)\n",
        "                break\n",
        "\n",
        "        return f\"translate InChI to IUPAC: {processed}\"\n",
        "\n",
        "    def preprocess_iupac(self, iupac: str) -> str:\n",
        "        \"\"\"Preprocess IUPAC with chemical markers\"\"\"\n",
        "        processed = iupac\n",
        "\n",
        "        # Mark stereochemistry\n",
        "        stereo_markers = ['(R)', '(S)', '(E)', '(Z)', 'cis', 'trans', 'fac', 'mer']\n",
        "        for marker in stereo_markers:\n",
        "            if marker in processed:\n",
        "                processed = processed.replace(marker, f'<STEREO> {marker}', 1)\n",
        "                break\n",
        "\n",
        "        # Mark haptic notation\n",
        "        haptic_markers = ['Œ∑¬π', 'Œ∑¬≤', 'Œ∑¬≥', 'Œ∑‚Å¥', 'Œ∑‚Åµ', 'Œ∑‚Å∂', 'Œ∑‚Å∑', 'Œ∑‚Å∏']\n",
        "        for marker in haptic_markers:\n",
        "            if marker in processed:\n",
        "                processed = processed.replace(marker, f'<HAPTIC> {marker}', 1)\n",
        "                break\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def encode(self, text: str, max_length: int, is_target: bool = False) -> Dict:\n",
        "        \"\"\"Encode text for T5\"\"\"\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "    def decode(self, token_ids: torch.Tensor) -> str:\n",
        "        \"\"\"Decode tokens to text\"\"\"\n",
        "        decoded = self.tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Clean special markers\n",
        "        markers = ['<METAL>', '<STANDARD>', '<RECONNECTED>', '<ORGANIC>',\n",
        "                  '<COORDINATION>', '<STEREO>', '<HAPTIC>']\n",
        "\n",
        "        for marker in markers:\n",
        "            decoded = decoded.replace(marker, '').strip()\n",
        "\n",
        "        return ' '.join(decoded.split())\n",
        "\n",
        "class InChI2NameDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for InChI-IUPAC pairs\"\"\"\n",
        "\n",
        "    def __init__(self, data: List[Tuple[str, str]], tokenizer: ChemicalT5Tokenizer, config: InChI2NameConfig):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inchi, iupac = self.data[idx]\n",
        "\n",
        "        # Preprocess and encode\n",
        "        processed_inchi = self.tokenizer.preprocess_inchi(inchi)\n",
        "        processed_iupac = self.tokenizer.preprocess_iupac(iupac)\n",
        "\n",
        "        # Encode input and target\n",
        "        input_encoding = self.tokenizer.encode(processed_inchi, self.config.max_input_length)\n",
        "        target_encoding = self.tokenizer.encode(processed_iupac, self.config.max_output_length, is_target=True)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_encoding['input_ids'],\n",
        "            'attention_mask': input_encoding['attention_mask'],\n",
        "            'labels': target_encoding['input_ids']\n",
        "        }\n",
        "\n",
        "class InChI2NameModel(nn.Module):\n",
        "    \"\"\"T5-based InChI to IUPAC name translation model\"\"\"\n",
        "\n",
        "    def __init__(self, config: InChI2NameConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        self.tokenizer = ChemicalT5Tokenizer(config.model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(config.model_name)\n",
        "\n",
        "        # Resize embeddings for new tokens\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer.tokenizer))\n",
        "\n",
        "        # Initialize validator\n",
        "        self.validator = ChemicalValidator()\n",
        "\n",
        "        print(f\"ü§ñ Model initialized: {config.model_name}\")\n",
        "        print(f\"   Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        return self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "    def predict(self, inchi: str, validate: bool = True) -> Dict:\n",
        "        \"\"\"Predict IUPAC name with optional validation\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Preprocess and encode\n",
        "            processed_inchi = self.tokenizer.preprocess_inchi(inchi)\n",
        "            encoding = self.tokenizer.encode(processed_inchi, self.config.max_input_length)\n",
        "\n",
        "            # Move to device\n",
        "            input_ids = encoding['input_ids'].unsqueeze(0).to(device)\n",
        "            attention_mask = encoding['attention_mask'].unsqueeze(0).to(device)\n",
        "\n",
        "            # Generate prediction\n",
        "            outputs = self.model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=self.config.max_output_length,\n",
        "                num_beams=self.config.num_beams,\n",
        "                do_sample=self.config.do_sample,\n",
        "                temperature=self.config.temperature,\n",
        "                early_stopping=True,\n",
        "                pad_token_id=self.tokenizer.tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            # Decode prediction\n",
        "            predicted_iupac = self.tokenizer.decode(outputs[0])\n",
        "\n",
        "            # Validate if requested\n",
        "            validation_result = None\n",
        "            if validate:\n",
        "                validation_result = self.validator.validate_prediction(inchi, predicted_iupac)\n",
        "\n",
        "            return {\n",
        "                'inchi': inchi,\n",
        "                'predicted_iupac': predicted_iupac,\n",
        "                'validation': validation_result\n",
        "            }\n",
        "\n",
        "# Initialize model\n",
        "print(\"üöÄ Initializing InChI2Name model...\")\n",
        "model = InChI2NameModel(config)\n",
        "model.to(device)\n",
        "\n",
        "# Test prediction\n",
        "test_inchi = \"InChI=1S/C6H6/c1-2-4-6-5-3-1/h1-6H\"\n",
        "test_result = model.predict(test_inchi)\n",
        "print(f\"\\nüß™ Test prediction:\")\n",
        "print(f\"   InChI: {test_result['inchi']}\")\n",
        "print(f\"   Predicted: {test_result['predicted_iupac']}\")\n",
        "if test_result['validation']:\n",
        "    print(f\"   Valid: {test_result['validation']['is_valid']}\")\n",
        "    print(f\"   Confidence: {test_result['validation']['confidence']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_section"
      },
      "source": [
        "##  Model Training and Optimization\n",
        "Train the model with optimized hyperparameters and early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_functions"
      },
      "outputs": [],
      "source": [
        "def split_dataset(data: List[Tuple[str, str]], config: InChI2NameConfig):\n",
        "    \"\"\"Split dataset into train/val/test sets\"\"\"\n",
        "    random.shuffle(data)\n",
        "\n",
        "    total_size = len(data)\n",
        "    train_size = int(total_size * config.train_split)\n",
        "    val_size = int(total_size * config.val_split)\n",
        "\n",
        "    train_data = data[:train_size]\n",
        "    val_data = data[train_size:train_size + val_size]\n",
        "    test_data = data[train_size + val_size:]\n",
        "\n",
        "    print(f\"üìä Dataset split:\")\n",
        "    print(f\"   Train: {len(train_data)} pairs\")\n",
        "    print(f\"   Validation: {len(val_data)} pairs\")\n",
        "    print(f\"   Test: {len(test_data)} pairs\")\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def create_data_loaders(train_data, val_data, model, config):\n",
        "    \"\"\"Create PyTorch data loaders\"\"\"\n",
        "    def collate_fn(batch):\n",
        "        return {\n",
        "            'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "            'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "            'labels': torch.stack([item['labels'] for item in batch])\n",
        "        }\n",
        "\n",
        "    train_dataset = InChI2NameDataset(train_data, model.tokenizer, config)\n",
        "    val_dataset = InChI2NameDataset(val_data, model.tokenizer, config) if val_data else None\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = None\n",
        "    if val_dataset:\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=0,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def train_model(model, train_loader, val_loader, config):\n",
        "    \"\"\"Train the model with optimized settings\"\"\"\n",
        "    print(\"üöÄ Starting model training...\")\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    optimizer = optim.AdamW(\n",
        "        model.model.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "\n",
        "    total_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation_steps\n",
        "    warmup_steps = int(total_steps * config.warmup_ratio)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training tracking\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    training_history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'learning_rates': []\n",
        "    }\n",
        "\n",
        "    print(f\"üìà Training configuration:\")\n",
        "    print(f\"   Total steps: {total_steps}\")\n",
        "    print(f\"   Warmup steps: {warmup_steps}\")\n",
        "    print(f\"   Learning rate: {config.learning_rate}\")\n",
        "    print(f\"   Batch size: {config.batch_size}\")\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        # Training phase\n",
        "        model.model.train()\n",
        "        train_loss = 0\n",
        "        train_steps = 0\n",
        "\n",
        "        progress_bar = tqdm(\n",
        "            train_loader,\n",
        "            desc=f\"Epoch {epoch+1}/{config.num_epochs}\",\n",
        "            leave=False\n",
        "        )\n",
        "\n",
        "        for step, batch in enumerate(progress_bar):\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss / config.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_steps += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            current_lr = scheduler.get_last_lr()[0] if scheduler.get_last_lr() else config.learning_rate\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'LR': f'{current_lr:.2e}'\n",
        "            })\n",
        "\n",
        "        # Calculate average training loss\n",
        "        avg_train_loss = train_loss / train_steps\n",
        "        training_history['train_loss'].append(avg_train_loss)\n",
        "        training_history['learning_rates'].append(current_lr)\n",
        "\n",
        "        # Validation phase\n",
        "        val_loss = float('inf')\n",
        "        if val_loader:\n",
        "            model.model.eval()\n",
        "            total_val_loss = 0\n",
        "            val_steps = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    labels = batch['labels'].to(device)\n",
        "\n",
        "                    outputs = model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels\n",
        "                    )\n",
        "\n",
        "                    total_val_loss += outputs.loss.item()\n",
        "                    val_steps += 1\n",
        "\n",
        "            val_loss = total_val_loss / val_steps\n",
        "            training_history['val_loss'].append(val_loss)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
        "        print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
        "        if val_loader:\n",
        "            print(f\"   Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"   Learning Rate: {current_lr:.2e}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save best model\n",
        "            if not os.path.exists(config.output_dir):\n",
        "                os.makedirs(config.output_dir)\n",
        "\n",
        "            torch.save({\n",
        "                'model_state_dict': model.model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_loss': val_loss,\n",
        "                'config': config\n",
        "            }, os.path.join(config.output_dir, 'best_model.pt'))\n",
        "\n",
        "            print(f\"üíæ New best model saved (Val Loss: {val_loss:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"‚è≥ No improvement for {patience_counter} epochs\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if patience_counter >= config.early_stopping_patience:\n",
        "            print(f\"üõë Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    print(\"‚úÖ Training completed!\")\n",
        "    return training_history\n",
        "\n",
        "# Split dataset\n",
        "train_data, val_data, test_data = split_dataset(data, config)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader, val_loader = create_data_loaders(train_data, val_data, model, config)\n",
        "\n",
        "# Train the model\n",
        "training_history = train_model(model, train_loader, val_loader, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation_section"
      },
      "source": [
        "##  Model Evaluation and Accuracy Calculation\n",
        "Comprehensive evaluation with multiple metrics including BLEU, exact match, and chemical validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation_functions"
      },
      "outputs": [],
      "source": [
        "class ModelEvaluator:\n",
        "    \"\"\"Comprehensive model evaluation system\"\"\"\n",
        "\n",
        "    def __init__(self, model, validator):\n",
        "        self.model = model\n",
        "        self.validator = validator\n",
        "\n",
        "    def evaluate_on_test_set(self, test_data: List[Tuple[str, str]]) -> Dict:\n",
        "        \"\"\"Evaluate model on test dataset\"\"\"\n",
        "        print(\"üîç Starting comprehensive evaluation...\")\n",
        "\n",
        "        results = {\n",
        "            'predictions': [],\n",
        "            'metrics': {\n",
        "                'exact_match': 0.0,\n",
        "                'bleu_score': 0.0,\n",
        "                'chemical_validity': 0.0,\n",
        "                'grammar_score': 0.0,\n",
        "                'metal_consistency': 0.0,\n",
        "                'confidence_score': 0.0\n",
        "            },\n",
        "            'by_category': {\n",
        "                'organic': {'count': 0, 'exact_match': 0, 'bleu': 0},\n",
        "                'organometallic': {'count': 0, 'exact_match': 0, 'bleu': 0},\n",
        "                'standard_inchi': {'count': 0, 'exact_match': 0, 'bleu': 0},\n",
        "                'reconnected_inchi': {'count': 0, 'exact_match': 0, 'bleu': 0}\n",
        "            }\n",
        "        }\n",
        "\n",
        "        exact_matches = 0\n",
        "        total_bleu = 0\n",
        "        total_chemical_validity = 0\n",
        "        total_grammar_score = 0\n",
        "        total_metal_consistency = 0\n",
        "        total_confidence = 0\n",
        "\n",
        "        self.model.model.eval()\n",
        "\n",
        "        for i, (inchi, true_iupac) in enumerate(tqdm(test_data, desc=\"Evaluating\")):\n",
        "            # Get prediction\n",
        "            prediction_result = self.model.predict(inchi, validate=True)\n",
        "            predicted_iupac = prediction_result['predicted_iupac']\n",
        "            validation = prediction_result['validation']\n",
        "\n",
        "            # Calculate metrics\n",
        "            is_exact_match = self._is_exact_match(predicted_iupac, true_iupac)\n",
        "            bleu = self._calculate_bleu(predicted_iupac, true_iupac)\n",
        "\n",
        "            # Aggregate metrics\n",
        "            if is_exact_match:\n",
        "                exact_matches += 1\n",
        "\n",
        "            total_bleu += bleu\n",
        "            total_chemical_validity += validation['chemical_consistency']\n",
        "            total_grammar_score += validation['grammar_score']\n",
        "            total_metal_consistency += validation['metal_consistency']\n",
        "            total_confidence += validation['confidence']\n",
        "\n",
        "            # Categorize results\n",
        "            self._categorize_result(inchi, predicted_iupac, true_iupac, is_exact_match, bleu, results)\n",
        "\n",
        "            # Store detailed results\n",
        "            results['predictions'].append({\n",
        "                'inchi': inchi,\n",
        "                'true_iupac': true_iupac,\n",
        "                'predicted_iupac': predicted_iupac,\n",
        "                'exact_match': is_exact_match,\n",
        "                'bleu_score': bleu,\n",
        "                'validation': validation\n",
        "            })\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        n_samples = len(test_data)\n",
        "        results['metrics']['exact_match'] = exact_matches / n_samples\n",
        "        results['metrics']['bleu_score'] = total_bleu / n_samples\n",
        "        results['metrics']['chemical_validity'] = total_chemical_validity / n_samples\n",
        "        results['metrics']['grammar_score'] = total_grammar_score / n_samples\n",
        "        results['metrics']['metal_consistency'] = total_metal_consistency / n_samples\n",
        "        results['metrics']['confidence_score'] = total_confidence / n_samples\n",
        "\n",
        "        # Calculate category-specific metrics\n",
        "        for category, stats in results['by_category'].items():\n",
        "            if stats['count'] > 0:\n",
        "                stats['exact_match'] = stats['exact_match'] / stats['count']\n",
        "                stats['bleu'] = stats['bleu'] / stats['count']\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _is_exact_match(self, predicted: str, true: str) -> bool:\n",
        "        \"\"\"Check if prediction exactly matches true value\"\"\"\n",
        "        return predicted.strip().lower() == true.strip().lower()\n",
        "\n",
        "    def _calculate_bleu(self, predicted: str, true: str) -> float:\n",
        "        \"\"\"Calculate BLEU score\"\"\"\n",
        "        try:\n",
        "            # Handle edge cases\n",
        "            if not predicted or not true:\n",
        "                return 0.0\n",
        "\n",
        "            # Clean and tokenize\n",
        "            predicted_tokens = predicted.strip().lower().split()\n",
        "            true_tokens = true.strip().lower().split()\n",
        "\n",
        "            if not predicted_tokens or not true_tokens:\n",
        "                return 0.0\n",
        "\n",
        "            # NLTK expects references as list of lists - THIS WAS THE MAIN ISSUE!\n",
        "            reference_list = [true_tokens]\n",
        "\n",
        "            # Use smoothing to avoid zero scores\n",
        "            smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "            # Calculate BLEU with smoothing\n",
        "            bleu_score = sentence_bleu(\n",
        "                reference_list,\n",
        "                predicted_tokens,\n",
        "                smoothing_function=smoothing_function\n",
        "            )\n",
        "\n",
        "            return max(0.0, min(1.0, bleu_score))\n",
        "\n",
        "        except Exception:\n",
        "            return 0.0\n",
        "\n",
        "    def _categorize_result(self, inchi: str, predicted: str, true: str,\n",
        "                          exact_match: bool, bleu: float, results: Dict):\n",
        "        \"\"\"Categorize results by molecule type and InChI format\"\"\"\n",
        "        # Check if organometallic\n",
        "        metals = ['Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Mn', 'Cr', 'Pt', 'Pd']\n",
        "        is_organometallic = any(metal in inchi for metal in metals)\n",
        "\n",
        "        # Check InChI format\n",
        "        is_standard = 'InChI=1S/' in inchi\n",
        "        is_reconnected = '/r' in inchi\n",
        "\n",
        "        # Update category statistics\n",
        "        if is_organometallic:\n",
        "            results['by_category']['organometallic']['count'] += 1\n",
        "            if exact_match:\n",
        "                results['by_category']['organometallic']['exact_match'] += 1\n",
        "            results['by_category']['organometallic']['bleu'] += bleu\n",
        "        else:\n",
        "            results['by_category']['organic']['count'] += 1\n",
        "            if exact_match:\n",
        "                results['by_category']['organic']['exact_match'] += 1\n",
        "            results['by_category']['organic']['bleu'] += bleu\n",
        "\n",
        "        if is_standard:\n",
        "            results['by_category']['standard_inchi']['count'] += 1\n",
        "            if exact_match:\n",
        "                results['by_category']['standard_inchi']['exact_match'] += 1\n",
        "            results['by_category']['standard_inchi']['bleu'] += bleu\n",
        "        elif is_reconnected:\n",
        "            results['by_category']['reconnected_inchi']['count'] += 1\n",
        "            if exact_match:\n",
        "                results['by_category']['reconnected_inchi']['exact_match'] += 1\n",
        "            results['by_category']['reconnected_inchi']['bleu'] += bleu\n",
        "\n",
        "    def generate_evaluation_report(self, results: Dict) -> str:\n",
        "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
        "        report = []\n",
        "        report.append(\"üéØ MODEL EVALUATION REPORT\")\n",
        "        report.append(\"=\" * 50)\n",
        "\n",
        "        # Overall metrics\n",
        "        metrics = results['metrics']\n",
        "        report.append(f\"\\nüìä Overall Performance:\")\n",
        "        report.append(f\"   Exact Match Accuracy: {metrics['exact_match']:.3f} ({metrics['exact_match']*100:.1f}%)\")\n",
        "        report.append(f\"   BLEU Score: {metrics['bleu_score']:.3f}\")\n",
        "        report.append(f\"   Chemical Validity: {metrics['chemical_validity']:.3f}\")\n",
        "        report.append(f\"   Grammar Score: {metrics['grammar_score']:.3f}\")\n",
        "        report.append(f\"   Metal Consistency: {metrics['metal_consistency']:.3f}\")\n",
        "        report.append(f\"   Confidence Score: {metrics['confidence_score']:.3f}\")\n",
        "\n",
        "        # Performance by category\n",
        "        report.append(f\"\\nüè∑Ô∏è Performance by Category:\")\n",
        "        for category, stats in results['by_category'].items():\n",
        "            if stats['count'] > 0:\n",
        "                report.append(f\"   {category.replace('_', ' ').title()}:\")\n",
        "                report.append(f\"     Count: {stats['count']}\")\n",
        "                report.append(f\"     Exact Match: {stats['exact_match']:.3f} ({stats['exact_match']*100:.1f}%)\")\n",
        "                report.append(f\"     BLEU Score: {stats['bleu']:.3f}\")\n",
        "\n",
        "        # Sample predictions\n",
        "        report.append(f\"\\nüìù Sample Predictions:\")\n",
        "        for i, pred in enumerate(results['predictions'][:5]):\n",
        "            status = \"‚úÖ\" if pred['exact_match'] else \"‚ùå\"\n",
        "            report.append(f\"   {i+1}. {status} {pred['true_iupac']} ‚Üí {pred['predicted_iupac']}\")\n",
        "            report.append(f\"      BLEU: {pred['bleu_score']:.3f}, Confidence: {pred['validation']['confidence']:.3f}\")\n",
        "\n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "print(\"üéØ Starting model evaluation...\")\n",
        "evaluator = ModelEvaluator(model, validator)\n",
        "evaluation_results = evaluator.evaluate_on_test_set(test_data[:50])  # Evaluate on subset for speed\n",
        "\n",
        "# Generate and display report\n",
        "report = evaluator.generate_evaluation_report(evaluation_results)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_section"
      },
      "source": [
        "##  Visualization and Analysis\n",
        "Visualize training progress and evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization_code"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(training_history):\n",
        "    \"\"\"Plot training and validation loss curves\"\"\"\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    epochs = range(1, len(training_history['train_loss']) + 1)\n",
        "\n",
        "    # Training and validation loss\n",
        "    ax1.plot(epochs, training_history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
        "    if training_history['val_loss']:\n",
        "        ax1.plot(epochs, training_history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
        "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning rate schedule\n",
        "    ax2.plot(epochs, training_history['learning_rates'], 'g-', linewidth=2)\n",
        "    ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Learning Rate')\n",
        "    ax2.set_yscale('log')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Model performance metrics\n",
        "    metrics_names = ['Exact Match', 'BLEU Score', 'Chemical Validity', 'Grammar Score']\n",
        "    metrics_values = [\n",
        "        evaluation_results['metrics']['exact_match'],\n",
        "        evaluation_results['metrics']['bleu_score'],\n",
        "        evaluation_results['metrics']['chemical_validity'],\n",
        "        evaluation_results['metrics']['grammar_score']\n",
        "    ]\n",
        "\n",
        "    bars = ax3.bar(metrics_names, metrics_values, color=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
        "    ax3.set_title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
        "    ax3.set_ylabel('Score')\n",
        "    ax3.set_ylim(0, 1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, metrics_values):\n",
        "        height = bar.get_height()\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Performance by category\n",
        "    categories = []\n",
        "    accuracies = []\n",
        "\n",
        "    for category, stats in evaluation_results['by_category'].items():\n",
        "        if stats['count'] > 0:\n",
        "            categories.append(category.replace('_', '\\n').title())\n",
        "            accuracies.append(stats['exact_match'])\n",
        "\n",
        "    if categories:\n",
        "        bars = ax4.bar(categories, accuracies, color=['lightcoral', 'lightblue', 'lightgreen', 'gold'])\n",
        "        ax4.set_title('Accuracy by Category', fontsize=14, fontweight='bold')\n",
        "        ax4.set_ylabel('Exact Match Accuracy')\n",
        "        ax4.set_ylim(0, 1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, value in zip(bars, accuracies):\n",
        "            height = bar.get_height()\n",
        "            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        plt.xticks(rotation=45)\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_prediction_examples_table(results):\n",
        "    \"\"\"Create a detailed table of prediction examples\"\"\"\n",
        "    print(\"\\nüìã DETAILED PREDICTION EXAMPLES\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    # Create DataFrame for better display\n",
        "    examples_data = []\n",
        "\n",
        "    for i, pred in enumerate(results['predictions'][:10]):\n",
        "        examples_data.append({\n",
        "            'ID': i + 1,\n",
        "            'True IUPAC': pred['true_iupac'][:30] + '...' if len(pred['true_iupac']) > 30 else pred['true_iupac'],\n",
        "            'Predicted IUPAC': pred['predicted_iupac'][:30] + '...' if len(pred['predicted_iupac']) > 30 else pred['predicted_iupac'],\n",
        "            'Match': '‚úÖ' if pred['exact_match'] else '‚ùå',\n",
        "            'BLEU': f\"{pred['bleu_score']:.3f}\",\n",
        "            'Confidence': f\"{pred['validation']['confidence']:.3f}\",\n",
        "            'Valid': '‚úÖ' if pred['validation']['is_valid'] else '‚ùå'\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(examples_data)\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "# Generate visualizations\n",
        "print(\"üìä Generating visualizations...\")\n",
        "plot_training_history(training_history)\n",
        "create_prediction_examples_table(evaluation_results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}